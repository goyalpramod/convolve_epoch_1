{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-20T19:57:23.220162Z","iopub.execute_input":"2022-12-20T19:57:23.220526Z","iopub.status.idle":"2022-12-20T19:57:23.230890Z","shell.execute_reply.started":"2022-12-20T19:57:23.220495Z","shell.execute_reply":"2022-12-20T19:57:23.229629Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/convolve-data/train.csv\n/kaggle/input/convolve-data/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:23.647360Z","iopub.execute_input":"2022-12-20T19:57:23.647826Z","iopub.status.idle":"2022-12-20T19:57:23.665407Z","shell.execute_reply.started":"2022-12-20T19:57:23.647787Z","shell.execute_reply":"2022-12-20T19:57:23.664462Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:24.387097Z","iopub.execute_input":"2022-12-20T19:57:24.387537Z","iopub.status.idle":"2022-12-20T19:57:24.397256Z","shell.execute_reply.started":"2022-12-20T19:57:24.387500Z","shell.execute_reply":"2022-12-20T19:57:24.395061Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/convolve-data/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:24.717564Z","iopub.execute_input":"2022-12-20T19:57:24.717932Z","iopub.status.idle":"2022-12-20T19:57:37.878999Z","shell.execute_reply.started":"2022-12-20T19:57:24.717900Z","shell.execute_reply":"2022-12-20T19:57:37.877896Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:37.880998Z","iopub.execute_input":"2022-12-20T19:57:37.881480Z","iopub.status.idle":"2022-12-20T19:57:37.888626Z","shell.execute_reply.started":"2022-12-20T19:57:37.881443Z","shell.execute_reply":"2022-12-20T19:57:37.887426Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"4152659"},"metadata":{}}]},{"cell_type":"code","source":"df.rename(columns = {'index':'log',0:'status'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:37.889948Z","iopub.execute_input":"2022-12-20T19:57:37.891026Z","iopub.status.idle":"2022-12-20T19:57:37.906800Z","shell.execute_reply.started":"2022-12-20T19:57:37.890989Z","shell.execute_reply":"2022-12-20T19:57:37.905800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:37.909485Z","iopub.execute_input":"2022-12-20T19:57:37.909945Z","iopub.status.idle":"2022-12-20T19:57:37.933402Z","shell.execute_reply.started":"2022-12-20T19:57:37.909909Z","shell.execute_reply":"2022-12-20T19:57:37.932086Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4152659 entries, 0 to 4152658\nData columns (total 2 columns):\n #   Column  Dtype \n---  ------  ----- \n 0   log     object\n 1   status  object\ndtypes: object(2)\nmemory usage: 63.4+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare the training data\ndf_parsed = df.copy(deep=True)\ndf_parsed[\"log\"]= df_parsed[\"log\"].str.split(\":\", n = 3, expand = False)\ndf_temp = df_parsed.copy(deep=True)\ndf_temp = pd.DataFrame(df_parsed['log'].to_list(), columns=['log1','log2','log3','log4'])\ndf_temp['status'] = df_parsed['status']\ndf4 = df_temp[df_temp['log4'].isnull()].copy(deep=True)\ndf5 = df_temp[df_temp['log4'].notnull()].copy(deep=True)\ntrain1 = df4.drop(['log4', 'log1', 'log2'], axis=1)\ntrain2 = df5.drop(['log3', 'log1', 'log2'], axis=1)\ntrain1.rename(columns = {'log3':'log'}, inplace = True)\ntrain2.rename(columns = {'log4':'log'}, inplace = True)\ntmp_list = [train1,train2]\ntrain = pd.concat(tmp_list)\ntrain = train.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:37.934907Z","iopub.execute_input":"2022-12-20T19:57:37.935407Z","iopub.status.idle":"2022-12-20T19:57:49.818280Z","shell.execute_reply.started":"2022-12-20T19:57:37.935369Z","shell.execute_reply":"2022-12-20T19:57:49.817308Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train[\"status\"] = train[\"status\"].map({\"abnormal\":0,\"normal\":1})","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:49.819850Z","iopub.execute_input":"2022-12-20T19:57:49.820244Z","iopub.status.idle":"2022-12-20T19:57:50.132396Z","shell.execute_reply.started":"2022-12-20T19:57:49.820206Z","shell.execute_reply":"2022-12-20T19:57:50.131427Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# method 1\n# doing a 50 50 split\n\ntrain_new1 = train.sample(frac=1)\n\nfraud_df = train_new1.loc[train_new1['status'] == 0]\nnon_fraud_df = train_new1.loc[train_new1['status'] == 1][:69692]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nbalanced_train1 = normal_distributed_df.sample(frac=1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:50.133836Z","iopub.execute_input":"2022-12-20T19:57:50.134683Z","iopub.status.idle":"2022-12-20T19:57:51.064884Z","shell.execute_reply.started":"2022-12-20T19:57:50.134632Z","shell.execute_reply":"2022-12-20T19:57:51.063895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# method 5, custom sampler\ntotal_normal = train[train[\"status\"] == 1]\ntotal_abnormal = train[train[\"status\"] == 0]\n\nsampled_total_normal = total_normal.sample(n=69692)\n\nnormal_distributed_df = pd.concat([total_abnormal, sampled_total_normal])\n\nbalanced_train2 = normal_distributed_df.sample(frac=1,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:51.066451Z","iopub.execute_input":"2022-12-20T19:57:51.066842Z","iopub.status.idle":"2022-12-20T19:57:51.406526Z","shell.execute_reply.started":"2022-12-20T19:57:51.066805Z","shell.execute_reply":"2022-12-20T19:57:51.405495Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# method 2\n# Over sampling- making copies of the minority class\n\nX = train.drop('status',axis=1)\ny = train['status']\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n#split data into test and training sets\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)\n#combine them back for resampling\ntrain_data = pd.concat([X_train, y_train], axis=1)\n# separate minority and majority classes\nnegative = train_data[train_data.status==0]\npositive = train_data[train_data.status==1]\n# upsample minority\npos_upsampled = resample(positive,\n replace=True, # sample with replacement\n n_samples=len(negative), # match number in majority class\n random_state=27) # reproducible results\n# combine majority and upsampled minority\nupsampled = pd.concat([negative, pos_upsampled])\n# check new class counts\nupsampled.status.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:51.408130Z","iopub.execute_input":"2022-12-20T19:57:51.408548Z","iopub.status.idle":"2022-12-20T19:57:52.973469Z","shell.execute_reply.started":"2022-12-20T19:57:51.408504Z","shell.execute_reply":"2022-12-20T19:57:52.972527Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0    46892\n1    46892\nName: status, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# method 3\n# downsample majority\n\nneg_downsampled = resample(negative,\n replace=True, # sample with replacement\n n_samples=len(positive), # match number in minority class\n random_state=27) # reproducible results\n# combine minority and downsampled majority\ndownsampled = pd.concat([positive, neg_downsampled])\n# check new class counts\ndownsampled.status.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:52.977919Z","iopub.execute_input":"2022-12-20T19:57:52.978192Z","iopub.status.idle":"2022-12-20T19:57:53.369945Z","shell.execute_reply.started":"2022-12-20T19:57:52.978167Z","shell.execute_reply":"2022-12-20T19:57:53.368823Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1    2676259\n0    2676259\nName: status, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/convolve-data/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:53.371310Z","iopub.execute_input":"2022-12-20T19:57:53.371790Z","iopub.status.idle":"2022-12-20T19:57:55.094861Z","shell.execute_reply.started":"2022-12-20T19:57:53.371750Z","shell.execute_reply":"2022-12-20T19:57:55.093696Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_parsed = test.copy(deep=True)\ndf_parsed[\" Log\"]= df_parsed[\" Log\"].str.split(\":\", n = 3, expand = False)\ndf_temp = df_parsed.copy(deep=True)\ndf_temp = pd.DataFrame(df_parsed[' Log'].to_list(), columns=['log1','log2','log3','log4'])\ndf_temp['ID'] = df_parsed['ID']\ndf4 = df_temp[df_temp['log4'].isnull()].copy(deep=True)\ndf5 = df_temp[df_temp['log4'].notnull()].copy(deep=True)\ntrain1 = df4.drop(['log4', 'log1', 'log2'], axis=1)\ntrain2 = df5.drop(['log3', 'log1', 'log2'], axis=1)\ntrain1.rename(columns = {'log3':'log'}, inplace = True)\ntrain2.rename(columns = {'log4':'log'}, inplace = True)\ntmp_list = [train1,train2]\ntemp_train = pd.concat(tmp_list)\ntest = temp_train\n# test = train.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:55.096336Z","iopub.execute_input":"2022-12-20T19:57:55.096755Z","iopub.status.idle":"2022-12-20T19:57:57.493861Z","shell.execute_reply.started":"2022-12-20T19:57:55.096710Z","shell.execute_reply":"2022-12-20T19:57:57.492599Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test = test.fillna(test.mode().iloc[0])","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:57.499328Z","iopub.execute_input":"2022-12-20T19:57:57.502052Z","iopub.status.idle":"2022-12-20T19:57:57.864673Z","shell.execute_reply.started":"2022-12-20T19:57:57.502014Z","shell.execute_reply":"2022-12-20T19:57:57.863588Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test = test.sort_values(\"ID\")","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:57.866194Z","iopub.execute_input":"2022-12-20T19:57:57.866580Z","iopub.status.idle":"2022-12-20T19:57:57.929700Z","shell.execute_reply.started":"2022-12-20T19:57:57.866540Z","shell.execute_reply":"2022-12-20T19:57:57.928684Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:57.931070Z","iopub.execute_input":"2022-12-20T19:57:57.931865Z","iopub.status.idle":"2022-12-20T19:57:57.939787Z","shell.execute_reply.started":"2022-12-20T19:57:57.931814Z","shell.execute_reply":"2022-12-20T19:57:57.938916Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:57:57.941125Z","iopub.execute_input":"2022-12-20T19:57:57.941585Z","iopub.status.idle":"2022-12-20T19:58:08.475053Z","shell.execute_reply.started":"2022-12-20T19:57:57.941550Z","shell.execute_reply":"2022-12-20T19:58:08.473780Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Create a function to tokenize a set of texts\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate/pad\n            pad_to_max_length=True,         # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True      # Return attention mask\n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:58:08.476910Z","iopub.execute_input":"2022-12-20T19:58:08.477311Z","iopub.status.idle":"2022-12-20T19:58:12.089522Z","shell.execute_reply.started":"2022-12-20T19:58:08.477270Z","shell.execute_reply":"2022-12-20T19:58:12.088380Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a8284168cc43db8274ec03c4cff9d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1881235e2af84cac81bc0b124711b8cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6622afbcda4d9580f9c731e1b545e0"}},"metadata":{}}]},{"cell_type":"code","source":"# Concatenate train data and test data\nall_tweets = np.concatenate([balanced_train2.log.values, test.log.values])\n\n# Encode our concatenated data\nencoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n\n# Find the maximum length\nmax_len = max([len(sent) for sent in encoded_tweets])\nprint('Max length: ', max_len)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T19:58:12.090933Z","iopub.execute_input":"2022-12-20T19:58:12.091386Z","iopub.status.idle":"2022-12-20T20:02:50.265493Z","shell.execute_reply.started":"2022-12-20T19:58:12.091347Z","shell.execute_reply":"2022-12-20T20:02:50.264470Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Max length:  220\n","output_type":"stream"}]},{"cell_type":"code","source":"X = balanced_train2.log.values\ny = balanced_train2.status.values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T20:02:50.267475Z","iopub.execute_input":"2022-12-20T20:02:50.268510Z","iopub.status.idle":"2022-12-20T20:02:50.583516Z","shell.execute_reply.started":"2022-12-20T20:02:50.268471Z","shell.execute_reply":"2022-12-20T20:02:50.582510Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import nltk\n# Uncomment to download \"stopwords\"\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\ndef text_preprocessing(s):\n    \"\"\"\n    - Lowercase the sentence\n    - Change \"'t\" to \"not\"\n    - Remove \"@name\"\n    - Isolate and remove punctuations except \"?\"\n    - Remove other special characters\n    - Remove stop words except \"not\" and \"can\"\n    - Remove trailing whitespace\n    \"\"\"\n    s = s.lower()\n    # Change 't to 'not'\n    s = re.sub(r\"\\'t\", \" not\", s)\n    # Remove @name\n    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n    # Isolate and remove punctuations except '?'\n    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n    # Remove some special characters\n    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n    # Remove stopwords except 'not' and 'can'\n    s = \" \".join([word for word in s.split()\n                  if word not in stopwords.words('english')\n                  or word in ['not', 'can']])\n    # Remove trailing whitespace\n    s = re.sub(r'\\s+', ' ', s).strip()\n    \n    return s","metadata":{"execution":{"iopub.status.busy":"2022-12-20T20:02:50.585044Z","iopub.execute_input":"2022-12-20T20:02:50.585436Z","iopub.status.idle":"2022-12-20T20:02:51.165914Z","shell.execute_reply.started":"2022-12-20T20:02:50.585398Z","shell.execute_reply":"2022-12-20T20:02:51.163710Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2022-12-20T20:02:51.167675Z","iopub.execute_input":"2022-12-20T20:02:51.168064Z","iopub.status.idle":"2022-12-20T20:02:52.907054Z","shell.execute_reply.started":"2022-12-20T20:02:51.168024Z","shell.execute_reply":"2022-12-20T20:02:52.906056Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Specify `MAX_LEN`\nMAX_LEN = 220\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-12-20T20:02:52.908684Z","iopub.execute_input":"2022-12-20T20:02:52.909535Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Original:  J03-U11 RAS KERNEL INFO iar 003a9294 dear 00b35ee8\n\nToken IDs:  [101, 1046, 2692, 2509, 1057, 14526, 20710, 16293, 18558, 24264, 2099, 4002, 2509, 2050, 2683, 24594, 2549, 6203, 4002, 2497, 19481, 4402, 2620, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTokenizing data...\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 128\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            #nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=1):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=3e-4,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# memory footprint support libraries/code\n!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n!pip install gputil\n!pip install psutil\n!pip install humanize\n\nimport psutil\nimport humanize\nimport os\nimport GPUtil as GPU\n\nGPUs = GPU.getGPUs()\n# XXX: only one GPU on Colab and isn’t guaranteed\ngpu = GPUs[0]\ndef printm():\n    process = psutil.Process(os.getpid())\n    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\nprintm()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n\nfrom GPUtil import showUtilization as gpu_usage\ngpu_usage() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numba import cuda\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=1)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, val_dataloader)\n\n# Evaluate the Bert classifier\nevaluate_roc(probs, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the train set and the validation set\nfull_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\nfull_train_sampler = RandomSampler(full_train_data)\nfull_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n\n# Train the Bert Classifier on the entire training data\nset_seed(42)\nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, full_train_dataloader, epochs=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run `preprocessing_for_bert` on the test set\nprint('Tokenizing data...')\ntest_inputs, test_masks = preprocessing_for_bert(test_data.tweet)\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\nthreshold = 0.9\npreds = np.where(probs[:, 1] > threshold, 1, 0)\n\n# Number of tweets predicted non-negative\nprint(\"Number of tweets predicted non-negative: \", preds.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}