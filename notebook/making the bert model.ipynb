{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2033bcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajag\\Desktop\\convolve\\convolve_epoch_1\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e813d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajag\\Desktop\\convolve\\convolve_epoch_1\\data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88f4c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6528093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom settings \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be30d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data \n",
    "with open('train.json', 'r') as f:\n",
    "    dict_train = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(dict_train, orient='index')\n",
    "df.reset_index(level=0, inplace=True)\n",
    "df.rename(columns = {'index':'log',0:'status'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296315da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "df_parsed = df.copy(deep=True)\n",
    "df_parsed[\"log\"]= df_parsed[\"log\"].str.split(\":\", n = 3, expand = False)\n",
    "df_temp = df_parsed.copy(deep=True)\n",
    "df_temp = pd.DataFrame(df_parsed['log'].to_list(), columns=['log1','log2','log3','log4'])\n",
    "df_temp['status'] = df_parsed['status']\n",
    "df4 = df_temp[df_temp['log4'].isnull()].copy(deep=True)\n",
    "df5 = df_temp[df_temp['log4'].notnull()].copy(deep=True)\n",
    "train1 = df4.drop(['log4', 'log1', 'log2'], axis=1)\n",
    "train2 = df5.drop(['log3', 'log1', 'log2'], axis=1)\n",
    "train1.rename(columns = {'log3':'log'}, inplace = True)\n",
    "train2.rename(columns = {'log4':'log'}, inplace = True)\n",
    "tmp_list = [train1,train2]\n",
    "train = pd.concat(tmp_list)\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c35b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 5, custom sampler\n",
    "total_normal = train[train[\"status\"] == \"normal\"]\n",
    "total_abnormal = train[train[\"status\"] == \"abnormal\"]\n",
    "\n",
    "sampled_total_normal = total_normal.sample(n=69692)\n",
    "\n",
    "normal_distributed_df = pd.concat([total_abnormal, sampled_total_normal])\n",
    "\n",
    "balanced_train2 = normal_distributed_df.sample(frac=1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d58d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fd10064",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c31f4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed = test.copy(deep=True)\n",
    "df_parsed[\" Log\"]= df_parsed[\" Log\"].str.split(\":\", n = 3, expand = False)\n",
    "df_temp = df_parsed.copy(deep=True)\n",
    "df_temp = pd.DataFrame(df_parsed[' Log'].to_list(), columns=['log1','log2','log3','log4'])\n",
    "df_temp['ID'] = df_parsed['ID']\n",
    "df4 = df_temp[df_temp['log4'].isnull()].copy(deep=True)\n",
    "df5 = df_temp[df_temp['log4'].notnull()].copy(deep=True)\n",
    "train1 = df4.drop(['log4', 'log1', 'log2'], axis=1)\n",
    "train2 = df5.drop(['log3', 'log1', 'log2'], axis=1)\n",
    "train1.rename(columns = {'log3':'log'}, inplace = True)\n",
    "train2.rename(columns = {'log4':'log'}, inplace = True)\n",
    "tmp_list = [train1,train2]\n",
    "train = pd.concat(tmp_list)\n",
    "test = train\n",
    "# test = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfbf6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54bcb0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993667058625902"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/multiprocess/\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tqdm-4.59.0.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (20.9)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-win_amd64.whl (20.3 MB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (5.4.1)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rajag\\appdata\\roaming\\python\\python38\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajag\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, fsspec, dill, xxhash, responses, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n"
     ]
    }
   ],
   "source": [
    "len(test)/len(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3509a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(test.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a57e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b08b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3131fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/timurrashitov/bert-transfer-learning-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc1050c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\urllib3-1.26.4.dist-info\\\\direct_url.json'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001A7BA72DA90>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/f9/2a/3ea9d3571e65b35dcbd86691e390eb518572f5938b8904cc9ddb8025878e/datasets-2.7.1-py3-none-any.whl\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'pyarrow'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 189, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 178, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 316, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 121, in resolve\n",
      "    self._result = resolver.resolve(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 453, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 347, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name, criterion)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 207, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_criteria_to_update(candidate)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 199, in _get_criteria_to_update\n",
      "    name, crit = self._merge_into_criterion(r, parent=candidate)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _merge_into_criterion\n",
      "    crit = Criterion.from_requirement(self._p, requirement, parent)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 82, in from_requirement\n",
      "    if not cands:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 124, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 38, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 167, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 300, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 144, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 226, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 311, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 457, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 480, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 230, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 108, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 163, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n",
      "'wandb' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet wandb\n",
    "!pip install --quiet transformers\n",
    "!pip install --quiet datasets\n",
    "!pip install --quiet emoji\n",
    "!pip install --quiet kaggle\n",
    "!pip install --quiet torchinfo\n",
    "!pip install --quiet imbalanced-learn\n",
    "!pip install --quiet gdown\n",
    "\n",
    "# my login - timakaznet, hust regirster wanbd account and u'll take trial period which is enough\n",
    "! wandb login fa19cfab8c0967bc35372f9f40932d67154809a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a236545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2b42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4c91ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e2792b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-a769a1a2779d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mSequence\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mValue\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mFeatures\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mClassLabel\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# data processing\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "# dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "from datasets import Dataset , Sequence , Value , Features , ClassLabel , DatasetDict\n",
    "\n",
    "# preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import re, string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17e2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of pretrained models: https://huggingface.co/transformers/v3.3.1/pretrained_models.html\n",
    "models = [\"distilbert-base-uncased\", \"bert-base-uncased\", \"bert-base-cased\"]\n",
    "modelName = models[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3216cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw df, should have only teo columns, log and status\n",
    "# Make the status 1 and 0\n",
    "df_train = df.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5d41f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['status'] = df_train['status'].map({\"normal\" : 1, \"abnormal\" : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7619128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.527847 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.823719 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.982731 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    log  \\\n",
       "0   1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n   \n",
       "1   1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.527847 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n   \n",
       "2   1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n   \n",
       "3   1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.823719 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n   \n",
       "4   1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.982731 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\\n   \n",
       "\n",
       "   status  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8641d971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log       0\n",
       "status    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "973aae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 101\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer\n",
    "df_train['log'] = df_train['log'].apply(lambda t: len(t.split()))\n",
    "min_len_word, max_len_word = df_train['log'].min(), df_train['log'].max()\n",
    "print(min_len_word, max_len_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "676bb3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kernel terminated for reason 1001\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J16-U01 RAS KERNEL FATAL data TLB error interrupt\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J16-U01 RAS KERNEL FATAL data TLB error interrupt\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>J03-U11 RAS KERNEL INFO generating core.6463\\n</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J16-U01 RAS KERNEL FATAL data TLB error interrupt\\n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   log  ID\n",
       "0                  kernel terminated for reason 1001\\n   0\n",
       "1  J16-U01 RAS KERNEL FATAL data TLB error interrupt\\n   1\n",
       "2  J16-U01 RAS KERNEL FATAL data TLB error interrupt\\n   2\n",
       "3       J03-U11 RAS KERNEL INFO generating core.6463\\n   3\n",
       "4  J16-U01 RAS KERNEL FATAL data TLB error interrupt\\n   4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24585a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the status 1 and 0\n",
    "df_test = test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c882579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4152659, 2) (595300, 2)\n",
      "(3778919, 2) (373740, 2) (595300, 2)\n"
     ]
    }
   ],
   "source": [
    "# suffle series\n",
    "RANDOM_SEED = 42\n",
    "df_train = df_train.sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "print(df_train.shape , df_test.shape)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.09, random_state=RANDOM_SEED, stratify=df_train['status'])\n",
    "print(df_train.shape , df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b38273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(df, textCol, labelCol):\n",
    "  dataset_dict = {\n",
    "    'text' : df[textCol],\n",
    "    'labels' : df[labelCol],\n",
    "  }\n",
    "  sent_tags = ClassLabel(num_classes=2 , names=['normal', 'abnormal'])\n",
    "\n",
    "  return Dataset.from_dict(\n",
    "    mapping = dataset_dict,\n",
    "    features = Features({'text' : Value(dtype='string') , 'labels' :sent_tags})\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1d40f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3926830</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802315</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573490</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984059</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494766</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         log  status\n",
       "3926830   18       1\n",
       "802315    13       1\n",
       "1573490   10       1\n",
       "1984059   10       1\n",
       "3494766   12       1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c4b6445",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassLabel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-67ca6e8c90ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"status\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"status\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# dataset_test = createDataset(df_test,\"Reviews\",\"Sentiment\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset_sentAnalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-aed455d8b9a1>\u001b[0m in \u001b[0;36mcreateDataset\u001b[1;34m(df, textCol, labelCol)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'labels'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   }\n\u001b[1;32m----> 6\u001b[1;33m   \u001b[0msent_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'abnormal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m   return Dataset.from_dict(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ClassLabel' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_train = createDataset(df_train,\"log\",\"status\")\n",
    "dataset_val = createDataset(df_val,\"log\",\"status\")\n",
    "# dataset_test = createDataset(df_test,\"Reviews\",\"Sentiment\")\n",
    "\n",
    "dataset_sentAnalysis = DatasetDict()\n",
    "dataset_sentAnalysis[\"train\"] = dataset_train\n",
    "dataset_sentAnalysis[\"val\"] = dataset_val\n",
    "# dataset_sentAnalysis[\"test\"] = dataset_test\n",
    "\n",
    "dataset_sentAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72792d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r\"<.*?>\",\"\",text)\n",
    "\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return wierd_pattern.sub(r'', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        lemma_word = lemmatizer.lemmatize(tokens[i])\n",
    "        tokens[i] = lemma_word\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc\n",
    "\n",
    "def preprocessText(text):\n",
    "  return remove_extra_white_spaces(remove_stopwords(remove_punctuation(remove_numbers(remove_emojis(convert_to_lower(text))))))\n",
    "\n",
    "def preprocessBatch(batch):\n",
    "  new_list = []\n",
    "  for i in batch[\"text\"]:\n",
    "    new_list.append(remove_extra_white_spaces(remove_stopwords(remove_punctuation(remove_numbers(remove_emojis(convert_to_lower(i)))))))\n",
    "  batch[\"text\"] = new_list\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentAnalysis_preprocessed = dataset_sentAnalysis.map(preprocessBatch, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentAnalysis[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentAnalysis_preprocessed[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentAnalysis_encoded = dataset_sentAnalysis_preprocessed.map(tokenize, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113ecba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentAnalysis_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertForClassification(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        # Load model body > return all og the HS\n",
    "        self.bert = BertModel(config)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "\n",
    "        # Apply classifier to encoder representation > [cls]\n",
    "        sequence_output = self.dropout(outputs[1])\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3eca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "id2label = {\n",
    "    0: 'Extremely Negative',\n",
    "    1: 'Negative',\n",
    "    2: 'Neutral',\n",
    "    3: 'Positive',\n",
    "    4: 'Extremely Positive'\n",
    "}\n",
    "\n",
    "label2id = { v:k for (k,v) in id2label.items()}\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(modelName, \n",
    "                                         num_labels=5,\n",
    "                                         id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042acf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = (BertForClassification\n",
    "              .from_pretrained(modelName, config=bert_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"bert-for-english-classification\")\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_sentAnalysis_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{modelName}-finetuned-sentimentAnalysis-bert\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  save_steps=1e6,\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False, \n",
    "                                  log_level=\"error\",\n",
    "                                  report_to=\"wandb\",\n",
    "                                  run_name=\"bert-sent-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f10cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * logging_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "trainer_preprocessed_lr = Trainer(model=bert_model, args=training_args,\n",
    "                                  compute_metrics=compute_metrics,\n",
    "                                  train_dataset=dataset_sentAnalysis_encoded[\"train\"],\n",
    "                                  eval_dataset=dataset_sentAnalysis_encoded[\"val\"],\n",
    "                                  tokenizer=tokenizer,\n",
    "                                  optimizers=(optimizer,lr_scheduler))\n",
    "\n",
    "trainer_preprocessed_lr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_model\n",
    "model.eval()\n",
    "preds_output = trainer_preprocessed_lr.predict(dataset_sentAnalysis_encoded[\"test\"])\n",
    "pd.DataFrame(list(preds_output.metrics.items())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749de163",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./bert-classification-classification-head\"\n",
    "torch.save(bert_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17dec18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_new = test.copy(deep=True)\n",
    "sub_new['status'] = pred\n",
    "sub_new.rename(columns = {'status':' Label'}, inplace = True)\n",
    "sub_new = sub_new.drop(['log'], axis=1)\n",
    "sub_new[\" Label\"] = sub_new[\" Label\"].map({0:\"abnormal\",1:\"normal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de37eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_new[\" Label\"] = sub_new[\" Label\"].map({0:\"abnormal\",1:\"normal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c60b8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_new= sub_new.loc[:,~sub_new.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2b61fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Label\n",
       "0   0  normal\n",
       "1   1  normal\n",
       "2   2  normal\n",
       "3   3  normal\n",
       "4   4  normal"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "627e7c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_new[sub_new[\" Label\"] == \"abnormal\"])/len(sub_new)\n",
    "# earlier 0.380398118595666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_new.to_csv(r'C:\\Users\\rajag\\Desktop\\convolve\\convolve_epoch_1\\submission\\submission_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a61af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
